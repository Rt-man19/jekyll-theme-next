---

title: 编译Hadoop支持Snappy和Lzo

categories:

- 大数据

tags:

---

# 编译Hadoop支持压缩

### 编译Snappy



**编译hadoop支持压缩的前提条件**:

```text
1. Unix 系统(本例centos6.5)
2. JDK 1.7+ 
3. Maven 3.0 或更高版本
4. Protobuf 2.5.0
5. CMake2.6 或更高版本
6. Zlib devel
7. openssl devel
8. 能访问互联网，或者使用本地仓库(本例使用本地仓库)
```

## 



#### 安装JDK&Maven&依赖环境

```shell
###JDK&Maven
$ tar zxfv jdk-7u80-linux-x64.tar.gz -C ~/usr/java/
$ tar zxfv apache-maven-3.6.0-bin.tar.gz -C ~/app/
$ tar zxfv repo.tar.gz /* 这是我保存过的本地仓库 */ -C ~/maven_repository/
$ vi ~/app/apache-maven/conf/settings.xml
## 添加
<localRepository>/home/hadoop/maven_repository/repo</localRepository>
$ vi ~/.bash_profile
### 添加以下内容
JAVA_HOME=/usr/java/jdk1.7.0_80
MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.0

export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH

### 环境依赖
$ yum -y install gcc gcc-c++ bzip2 bzip2-devel zlib zlib-devel lzo lzo-devel lzop autoconf automake curl curl-devel openssl openssl-devel svn ncurses-devel libtool 

```

#### 安装CMake&Protobuf&Snappy

```shell
###CMake
### CMake编译需要C++11，但是Centos6.5默认的gcc版本不支持c++11,所以先编译gcc和glibc
$ tar jxvf gcc-4.8.1.tar.bz2 -C ~/app/
$ tar jxf gmp-4.3.2.tar.bz2 
$ tar jxf mpfr-2.4.2.tar.bz2
$ tar zxf mpc-0.8.1.tar.gz
$ mv gmp-4.3.2 ~/app/gcc-4.8.1/gmp
$ mv mpfr-2.4.2 ~/app/gcc-4.8.1/mpfr
$ mv mpc-0.8.1 ~/app/gcc-4.8.1/mpc
$ cd ~/app/gcc-4.8.1/ && mkdir build && cd build
$ ../configure --prefix=/usr/local/gcc4.8.1 --enable-languages=c,c++ --enable-checking=release --disable-multilib
## --prefix是可选项,如果想保留系统原有的gcc版本，请添加这个选项
$ make -j4
$ make install
###glibc
$ tar zxfv glibc-2.14.tar.gz -C ~/app/
$ cd ~/app/glibc-2.14 && mkdir build && cd build
$ ../configure --prefix=/usr/local/glibc2.14
$ make -j4 
$ make install
$ ln -s /usr/local/lib64/libstdc++.* /usr/lib64/

### CMake
$ tar zxfv cmake-3.14.1.tar.gz -C ~/app/
$ cd ~/app/cmake-3.14.1
$ ./bootstrap && make && make install

### Protobuf
$ tar zxfv protobuf-2.5.0.tar.gz -C ~/app/
$ cd ~/app/protobuf-2.5.0/
$ ./configure --prefix=/usr/local/protobuf && make && make install
### snappy 
$ tar zxfv snappy-1.1.4.tar.gz ~/app/
$ cd ~/app/snappy-1.1.4
$ ./configure && make && make install
### 环境变量
$ vi /home/hadoop/.bash_profile
##添加
export PROTOBUF_HOME=/usr/local/protobuf
export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PROTOBUF_HOME/bin:$PATH
```

#### 编译hadoop

```shell
### 下载源码包，解压
$ tar zxfv hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/app/
$ cd ~/app/hadoop-2.6.0-cdh5.7.0
$ mvn clean package -Pdist,native -DskipTests -Dtar -Dhttps.protocols=TLSv1.2
### 在编译完成后会提示BUILD SUCCESS字样
### 编译完成后执行
$ mv ~/app/hadoop-2.6.0-cdh5.7.0/hadoop-dist/hadoop-2.6.0-cdh5.7.0.tar.gz ~/software
$ mkdir ~/app/hadoop/
$ tar zxfv ~/software/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/hadoop/
$ vi ~/.bash_profile
### 添加以下字段
export HADOOP_HOME=/home/hadoop/app/hadoop/hadoop2.6.0-cdh5.7.0
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:"$JAVA_HOME/bin:$MAVEN_HOME/bin:$PROTOBUF_HOME/bin:$PATH
$ source ~/.bash_profile
$ ln -s /usr/local/lib/libsnappy.so.1 /root/app/hadoop/hadoop2.6.0-cdh5.7.0/lib/native
$ hadoop checknative -a
查看五种压缩都为true就是编译成功啦
```

![](https://s2.ax1x.com/2019/04/06/AWLCin.png)

### 编译支持LZO

##### 安装LZO

```shell
$ wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz
$ tar zxfv lzo-2.10.tar.gz -C app/
$ cd app/lzo-2.10
$ ./configure --enable-shared --prefix=/usr/local/lzo
$ make && make install
```

##### 编译hadoop-lzo

```shell
$ git clone https://github.com/twitter/hadoop-lzo.git
$ export C_INCLUDE_PATH=/usr/local/lzo/include
$ export LIBRARY_PATH=/usr/local/lzo/lib
$ cd hadoop-lzo
$ mvn clean package
$ cd target/native/Linux-amd64-64
$ tar -cBf – -C lib . | tar -xBvf – -C ~
$ cp ~/libgplcompression* $HADOOP_HOME/lib/native/
$ cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/
```



##### 测试

```xml

<!-- 编辑hadoop-env.sh -->
	export LD_LIBRARY_PATH=/usr/local/lzo/lib
<!-- 编辑core-site.xml -->
<!-- 添加 -->
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:8020</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/home/hadoop/app/tmp</value>
  </property>
  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,
           org.apache.hadoop.io.compress.DefaultCodec,
           org.apache.hadoop.io.compress.Bzip2Codec,
           com.hadoop.compression.lzo.LzoCodec,
           com.hadoop.compression.lzo.LzopCodec
    </value>
  </property>
  <property>
	<name>io.compression.codec.lzo.class</name>
	<value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>
</configuration>
<!-- 编辑hdfs-site.xml -->
<!-- 我本机是单点测试用的所以只有一个副本 -->
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
<!-- 编辑mapred-site.xml -->
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
  	<name>mapreduce.map.output.compress</name>
  	<value>true</value>
  </property>
  <property>
    <name>mapreduce.map.output.compress.codec</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
  <property>
    <name>mapreduce.output.fileoutputformat.compress</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.output.fileoutputformat.compress</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>
</configuration>
<!-- 编辑yarn-site.xml -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
```

###### 不创建索引

```shell

## 我的block size更改为了: 10M,下面在Hive中创建Lzo压缩测试表
hive>create table page_views2_lzo(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
) row format delimited fields terminated by '\t'
STORED AS INPUTFORMAT "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat";
hive> LOAD DATA LOCAL INPATH '/root/page_views.dat.lzo' OVERWRITE INTO TABLE page_views2_lzo;
$ hadoop fs  -du -s -h /user/hive/warehouse/page_views2_lzo
34.5 M  34.5 M  /user/hive/warehouse/page_views2_lzo

hive> select count(1) from page_views2_lzo;
可以看到这时只起了一个MAP TASK，这时因为LZO 默认是不支持分片的需要通过创建索引来使其支持分片
```

![](https://s2.ax1x.com/2019/04/16/AxyLRJ.png)

###### 创建索引

```shell
hive> create table page_views2_lzo_split
STORED AS INPUTFORMAT "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
as select *  from page_views2_lzo;
hadoop fs -du -s -h /user/hive/warehouse/page_views2_lzo_split/*
34.2 M  34.2 M  /user/hive/warehouse/page_views2_lzo_split/000000_0.lzo
## 创建索引
$ hadoop jar ~/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.LzoIndexer /user/hive/warehouse/page_views2_lzo_split
## 执行成功后会在page_views2_lzo_split的目录里创建后缀为.index的文件
$ hadoop fs -du -s -h /user/hive/warehouse/page_views2_lzo_split/*
34.2 M  34.2 M  /user/hive/warehouse/page_views2_lzo_split/000000_0.lzo
2.4 K  2.4 K  /user/hive/warehouse/page_views2_lzo_split/000000_0.lzo.index
hive> select count(1) from page_views2_lzo_split;
```

![](https://s2.ax1x.com/2019/04/16/Ax61zj.png)

