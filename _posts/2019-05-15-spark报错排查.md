**不定期更新**

1. WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME

> 在运行spark-shell --master=yarn时出现如上警告信息，这是由于spark on yarn模式下，提交作业时yarn会将spark jars分发到yarn的container中，很消耗资源

解决方法: spark.yarn.jars

> ```shell
> $ hadoop fs -mkdir -p /spark/jars
> $ hadoop fs -put /root/app/spark/jars/* /spark/jars/
> $ vi /root/app/spark/conf/spark-defaults.conf
> ## 添加spark.yarn.jars配置项
> spark.yan.jars=hdfs://hadoop004:8020/spark/jars/*
> ```
>
> 之后再运行spark-shell --master=yarn，这条警告就不见了

2. Exception in thread "main" org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells

> 这条信息是在执行spark-shell --master=yarn --deploy-mode=cluster之后出现的。
>
> 报错信息显示，spark-shell不支持cluster模式?这是为什么?
>
> 因为spark-shell作为一个与用户交互的命令行，必须将Driver运行在本地，而不是yarn上。
>
> 所以spark-shell不能使用--deploy-mode=cluster， 同理spark-sql
>
> 这一条在源码中可以找到相关的代码片段:
>
> ```scala
> // SparkSubmit.scala
>     // Fail fast, the following modes are not supported or applicable
>     (clusterManager, deployMode) match {
>       case (STANDALONE, CLUSTER) if args.isPython =>
>         error("Cluster deploy mode is currently not supported for python " +
>           "applications on standalone clusters.")
>       case (STANDALONE, CLUSTER) if args.isR =>
>         error("Cluster deploy mode is currently not supported for R " +
>           "applications on standalone clusters.")
>       case (LOCAL, CLUSTER) =>
>         error("Cluster deploy mode is not compatible with master \"local\"")
>       case (_, CLUSTER) if isShell(args.primaryResource) =>
>         error("Cluster deploy mode is not applicable to Spark shells.")
>       case (_, CLUSTER) if isSqlShell(args.mainClass) =>
>         error("Cluster deploy mode is not applicable to Spark SQL shell.")
>       case (_, CLUSTER) if isThriftServer(args.mainClass) =>
>         error("Cluster deploy mode is not applicable to Spark Thrift server.")
>       case _ =>
>     }
> ```
