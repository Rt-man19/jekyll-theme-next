---
title: SparkSQL之外部数据源
tags:
  - 大数据
  - Spark
categories:
  - 大数据
---
# 加载数据的方法

SparkSQL支持多种数据格式，如JSON/CSV/JDBC等...

Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。

Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项`spark.sql.sources.default`，可修改默认数据源格式。

在源码中的体现

```scala
  // This is used to set the default data source
  val DEFAULT_DATA_SOURCE_NAME = buildConf("spark.sql.sources.default")
    .doc("The default data source to use in input/output.")
    .stringConf
    .createWithDefault("parquet")
```

## 加载数据的方式

简写

```scala
scala> spark.read.parquet("PATH")
```

全写

```scala
scala> spark.read.format("parquet").load("PATH")
```

其实在源码中，简写底层调用的就是全写的方式。比如parquet

```scala
  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }
// 这里的paths是可以传多个的
```



## 加载Parquet文件

```scala
scala> val df = spark.read.format("parquet").load("PATH","PATHS")
scala> val df2 = spark.read.parquet("PATH","PATHS")
```

在加载完成后就可以对其进行查询等操作了

##  加载其他格式的文件

如果数据源格式不是Parquet，需要手动指定数据源的格式(数据源格式要写成包名的形式如: org.apache.spark.sql.parquet). 但如果是内置支持的格式的话可以直接使用shortName指定即可

内置数据源格式有csv、jdbc、json、orc、parquet、text

比如JSON数据源

```scala
scala> spark.read.json("PATH")
scala> spark.read.format("json").load("PATH")
```



# 保存数据的方法

通用保存方式`df.write.format("format").save("PATH")`

例子: 

```scala
scala>  val peopleDF = spark.read.format("json").load("file:///root/app/spark/examples/src/main/resources/people.json")
scala> peopleDF.write.format("parquet").save("file:///root/tmp/peopledf")
```



## 保存选项：Save Mode

指定SaveMode来决定如何处理现有数据，这些保存选项不使用任何所，操作也不是原子性的。此外如果执行Overwrite操作时，在写入新的数据之前会将之前的数据删除

| Scala/Java                        | Any Language                          | Meaning                                                      |
| --------------------------------- | ------------------------------------- | ------------------------------------------------------------ |
| `SaveMode.ErrorIfExists`(default) | `"error" or "errorifexists"`(default) | 如果数据已经存在，则报错                                     |
| SaveMode.Append                   | "append"                              | 如果数据或表已经存在，则将需要保存的数据追加到已经存在的数据或表中 |
| SaveMode.Overwrite                | "overwrite"                           | 如果数据或表已经存在了，那么会把本次保存的数据覆盖之前已经存在的数据 |
| SaveMode.Ignore                   | "ignore"                              | 如果数据或表已经存在，那么保存操作就被忽略，这和SQL中的CREATE TABLE IF NOT EXIST 类似 |



# Hive数据源



> ORC、Parquet、JSON官网都有非常详细的描述，文章中不再赘述，只写一些我个人觉得常用或容易出错地方
>
> 官网地址： <http://spark.apache.org/docs/latest/sql-programming-guide.html>

SparkSQL还支持读取/写入那些存储在Hive中的数据，但是Hive有大量的依赖，这些依赖默认Spark 发行版是不包含的，所以一般都是下载Spark源码包自行编译，从而添加Hive的依赖。

如果Spark SQL要连接已经安装好的Hive，必须把`hive-site.xml`复制到$SPARK_HOME/conf 下面，这样即使Spark执行的那台机器没有Hive，Spark也可以连接到其他部署好的Hive上。但是这种方法需要将mysql-connector-java-VERSION.jar复制到$SPARK_HOME/lib目录下，或者在运行作业时使用`spark-submit --jars` 引入这个jar包

如果Spark要和Hive一起工作，那么要在SparkSession中开启Hive依赖

```scala
  def main(args: Array[String]): Unit = {
    val spark = new SparkSession.Builder()
      .master("local[2]")
      .appName("DataSourceApp")
      .enableHiveSupport() // 添加Hive依赖
      .getOrCreate()
  }
```

另外如果没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。

如果使用的是内置Hive,需要添加一项配置`spark.sql.warehouse.dir`,在Spark2.0之后这项配置用于指定数据仓库的地址，如果使用的是HDFS路径，则需要将`core-site.xml`和`hdfs-site.xml`复制到$SPARK_HOME/conf下。



## 官网案例

```scala
import java.io.File

import org.apache.spark.sql.{Row, SaveMode, SparkSession}

case class Record(key: Int, value: String)

// warehouseLocation points to the default location for managed databases and tables
val warehouseLocation = new File("spark-warehouse").getAbsolutePath

val spark = SparkSession
  .builder()
  .appName("Spark Hive Example")
  .config("spark.sql.warehouse.dir", warehouseLocation)
  .enableHiveSupport()
  .getOrCreate()

import spark.implicits._
import spark.sql

sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive")
sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

// Queries are expressed in HiveQL
sql("SELECT * FROM src").show()
// +---+-------+
// |key|  value|
// +---+-------+
// |238|val_238|
// | 86| val_86|
// |311|val_311|
// ...

// Aggregation queries are also supported.
sql("SELECT COUNT(*) FROM src").show()
// +--------+
// |count(1)|
// +--------+
// |    500 |
// +--------+

// The results of SQL queries are themselves DataFrames and support all normal functions.
val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")

// The items in DataFrames are of type Row, which allows you to access each column by ordinal.
val stringsDS = sqlDF.map {
  case Row(key: Int, value: String) => s"Key: $key, Value: $value"
}
stringsDS.show()
// +--------------------+
// |               value|
// +--------------------+
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// ...

// You can also use DataFrames to create temporary views within a SparkSession.
val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
recordsDF.createOrReplaceTempView("records")

// Queries can then join DataFrame data with data stored in Hive.
sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()
// +---+------+---+------+
// |key| value|key| value|
// +---+------+---+------+
// |  2| val_2|  2| val_2|
// |  4| val_4|  4| val_4|
// |  5| val_5|  5| val_5|
// ...

// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax
// `USING hive`
sql("CREATE TABLE hive_records(key int, value string) STORED AS PARQUET")
// Save DataFrame to the Hive managed table
val df = spark.table("src")
df.write.mode(SaveMode.Overwrite).saveAsTable("hive_records")
// After insertion, the Hive managed table has data now
sql("SELECT * FROM hive_records").show()
// +---+-------+
// |key|  value|
// +---+-------+
// |238|val_238|
// | 86| val_86|
// |311|val_311|
// ...

// Prepare a Parquet data directory
val dataDir = "/tmp/parquet_data"
spark.range(10).write.parquet(dataDir)
// Create a Hive external Parquet table
sql(s"CREATE EXTERNAL TABLE hive_ints(key int) STORED AS PARQUET LOCATION '$dataDir'")
// The Hive external table should already have data
sql("SELECT * FROM hive_ints").show()
// +---+
// |key|
// +---+
// |  0|
// |  1|
// |  2|
// ...

// Turn on flag for Hive Dynamic Partitioning
spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")
spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")
// Create a Hive partitioned table using DataFrame API
df.write.partitionBy("key").format("hive").saveAsTable("hive_part_tbl")
// Partitioned column `key` will be moved to the end of the schema.
sql("SELECT * FROM hive_part_tbl").show()
// +-------+---+
// |  value|key|
// +-------+---+
// |val_238|238|
// | val_86| 86|
// |val_311|311|
// ...

spark.stop()
```



# JDBC

JDBC数据源全称: org.apache.spark.sql.execution.datasources.jdbc

SparkSQL可以使用JDBC数据源直接读取关系型数据库中的表并转为DataFrame或者一张临时表，经过一系列计算之后再把数据写回去。用户可以通过option来指定JDBC连接的一些信息，像`user`，`password`这些属性都可以通过option指定,但SparkSQL还支持很多其他的属性,详细如下:

| **Property Name**                       | **Meaning**                                                  |
| --------------------------------------- | ------------------------------------------------------------ |
| url                                     | 指定需要连接的数据库地址,如: jdbc:mysql://hadoop000:3306/sparksql |
| dbtable                                 | 指定需要读取或写入的表                                       |
| query                                   | 指定一项查询，将查询的数据作为Spark作业的数据。但是，这个选项不能和`dbtable`、`numPartitions`同时出现!! |
| driver                                  | 指定用来连接数据库的JDBC包                                   |
| partitionColumn, lowerBound, upperBound | 如果指定了其中一个，那么这三个选项必须同时出现!!此外,如果需要使用这些选项，那么`numPartitions`是必须指定的。这些选项描述了在并行读取数据时如何对表分区。partitionColumn必须是数字/时间/时间戳的列。注意，下界和上界只是用来决定分区步长，而不是用于过滤表中的行。因此表中的所有行都将被分区并返回。所以选项仅适用于读取。 |
| numPartitions                           | 指定并行读取或写入数据时的最大并行度,也可以理解为当前最多能有多少JDBC 连接，如果要写入的分区大于这个选项的值，那么会在写入之前调用`coalesce(numpartition)`将其减少到这个数值 |
| queryTimeout                            | 指定一次读取或写入能等待的最大时间，0表示没有限制，默认值是0 |
| fetchsize                               | 指定每次读取多少行，只对读取生效                             |
| batchsize                               | 指定每次写入多少行，只对写入生效，默认值1000                 |
| isolationLevel                          | 指定事务隔离级别，只对当前连接生效它可以是NONE、READ_COMMITTED、READ_UNCOMMITTED、REPEATABLE_READ或SERIALIZABLE中的一个，默认值。默认值为READ_UNCOMMITTED。这个选项只适用于写入操作。 |
| sessionInitStatement                    | 在连接数据库读取数据之前，可以使用这个选项执行一个自定义SQL语句，例如:`option("sessionInitStatement", """BEGIN execute immediate 'alter session set "_serial_direct_read"=true'; END;""")` |
| truncate                                | 当`SaveMode.Overwrite`开启时，这个选项可以使Spark截断当前表而不是删除或重建这个表，这个选项只对写操作生效，默认值false |
| cascadeTruncate                         | 慎用! 这个选项允许执行`TRUNCATE TABLE t CASCADE`，这会影响到其他表，慎用慎用。只对写操作生效 |
| createTableOptions                      | 如果指定，此选项允许在创建表时设置特定于数据库的表和分区选项，只对写操作生效 |
| createTableColumnTypes                  | 创建表时指定列的类型，不使用默认类型。                       |
| customSchema                            | 指定自定义schema                                             |
| pushDownPredicate                       | 是否开启谓词下推，默认值true                                 |



注意: 操作数据库时,需要引入相关的jar包，如: 操作MySQL数据库，需要引入mysql-connector-java-VERSION.jar 包。可以放到$SPARK_HOME/lib目录下或者使用`spark-submit --jars` 引入

另外建议在把数据写回去的时候使用foreachPartition算子替换foreach算子，详情参考优化指南.

```scala
// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
// Loading data from a JDBC source
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:postgresql:dbserver")
  .option("dbtable", "schema.tablename")
  .option("user", "username")
  .option("password", "password")
  .load()

val connectionProperties = new Properties()
connectionProperties.put("user", "username")
connectionProperties.put("password", "password")
val jdbcDF2 = spark.read
  .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
// Specifying the custom data types of the read schema
connectionProperties.put("customSchema", "id DECIMAL(38, 0), name STRING")
val jdbcDF3 = spark.read
  .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)

// Saving data to a JDBC source
jdbcDF.write
  .format("jdbc")
  .option("url", "jdbc:postgresql:dbserver")
  .option("dbtable", "schema.tablename")
  .option("user", "username")
  .option("password", "password")
  .save()

jdbcDF2.write
  .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)

// Specifying create table column data types on write
jdbcDF.write
  .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
  .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
```

