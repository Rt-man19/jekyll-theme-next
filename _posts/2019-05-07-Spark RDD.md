---

title: Spark RDD简介

categories:

- Spark

tags:

---

# RDD 

## 简介

RDD(Resilient Distributed Dataset)，弹性分布式数据集,Spark中最基本的抽象。它通常通过其他RDD转换而成，而RDD也提供了丰富的转换操作 。

RDD作为数据结构本质上是一个只读的分区记录集合，一个RDD可以包含多个分区.

## RDD特点

### 分区

RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute方法得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute方法是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute方法是执行转换逻辑将其他RDD的数据进行转换。

**compute方法**

```scala
// RDD.scala
...
/**
   * :: DeveloperApi ::
   * Implemented by subclasses to compute a given partition.
   */
  @DeveloperApi
  def compute(split: Partition, context: TaskContext): Iterator[T]
```

从源码中可以看出,该方法介绍的参数之一split就是一个Partition对象，也就是分区对象。并且RDD抽象类要求所有子类中必须实现compute方法，比如mapPartitions中的compute

```scala
override def compute(split: Partition, context: TaskContext): Iterator[U] =
  f(context, split.index, firstParent[T].iterator(split, context))
```

mapPartitions中的compute方法调用当前RDD内第一个父RDD的iterator方法，目的是拉取父RDD对应分区的数据，iterator方法返回一个迭代器对象,内部存储的每一个元素即是父RDD对应分区内的数据记录。

f函数就是map转换操作函数, RDD会对一个分区内的数据执行操作f，最终返回包含所有经过转换过的数据记录的新迭代器，也就是一个新的分区，这也解释了RDD可以通过其他RDD创建。

### 不可变

RDD是只读的，想要改变RDD中的数据只能通过现有的RDD创建新的RDD

从一个RDD转换为另一个RDD，这种操作在RDD中提供了丰富的算子来实现，RDD的操作算子包含两类

- transformations

  他是用来将RDD进行转换，构建RDD依赖关系的

- actions

  他是用来触发RDD的计算，得到RDD的相关计算结果或者保存到文件中

#### 常用transformations算子

| 算子                                                    | 含义                                                         |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| map(func)                                               | 通过func传进来的每个元素来返回一个新的分布式数据集           |
| filter(func)                                            | 当func为true时返回一个新的分布式数据集                       |
| flatMap(func)                                           | 和map相似，但是对每个输入项都可以返回一个或多个新元素        |
| mapPartitions(func)                                     | 和map相似,但是它是在每个分区块上单独运行，所以当在类型为T的RDD上运行时`func`必须是`Iterator<T> => Iterator<U>` |
| reduceByKey(func,[numPartitions])                       | 对每个key对应的value做reduce操作                             |
| groupByKey([numPartitions])                             | 根据key进行分组，返回一个`（K,Iterator<V>）`键值对类型的数据集 |
| aggregateByKey(zeroValue)(seqOp,combOp,[numPartitions]) | 与groupByKey类似，当对(K, V)对的数据集调用时，返回(K, U)键值对的数据集，其中每个键的值使用给定的combine函数和一个初始值"0"进行聚合。 |
| sortByKey([ascending],[numPartitions])                  | 对每个key对应的value做排序操作                               |
| repartition(numPartitions)                              | 对RDD中的数据重新shuffle,创建更多或更少的分区                |
| repartitionAndSortWithinPartitions                      | 根据给定的分区重新分区，并在返回的分区中根据键值对记录进行排序 |

#### 常用action算子

| 算子                 | 含义                                                         |
| -------------------- | ------------------------------------------------------------ |
| reduce(func)         | 对RDD中的元素进行聚合操作，第一个元素和第二个元素聚合，返回一个新元素，并与第三个元素聚合。。。以此类推 |
| collect()            | 返回Driver程序中的所有元素                                   |
| count()              | 获取RDD元素的个数                                            |
| firts()              | 返回RDD中第一个元素,效果和take(1)一样                        |
| take(n)              | 获取RDD中的前n个元素                                         |
| saveAsTextFile(PATH) | 将RDD中的元素保存到文件系统中。类似的还有saveAsSequenceFile()和saveAsObjectFile() |
| foreach(func)        | 遍历RDD中的每个元素并执行func                                |
| countByKey           | 对每个key对应值进行count操作                                 |

### 依赖

通过算子转换得到的新RDD包含了所有父RDD衍生所必须的信息，它们之间维护这这种血缘关系，也成为依赖关系。

依赖包括两种:

- 窄依赖

  父子RDD之间分区是意义对应的

- 宽依赖

  子RDD分区和父RDD的每个分区都有关系，是多对多的关系

### 缓存

在使用RDD的时候，第一次计算RDD时会根据依赖关系得到分区的数据，之后可以将该RDD缓存起来，在其他地方再次用到该RDD的时候就直接从缓存中取，不需要重新计算依赖关系。这样有利于复用。

### checkpoint

RDD的依赖关系使其自然支持容错，当某个RDD分区数据丢失时可以通过依赖关系计算实现重建。但当RDD迭代次数过多，RDD的依赖关系也越来越长，后续一旦迭代过程中出错就需要很长的时间去重建，这很影响性能。RDD支持checkpoint将数据保存到持久化存储中，这样就可以切断之前的依赖关系。

## 运行架构

下面这些内容来自官网内容翻译: [Spark Cluster Mode OverView](<http://spark.apache.org/docs/latest/cluster-overview.html>)

### 专业术语

| 名称            | 含义                                                         |
| --------------- | ------------------------------------------------------------ |
| Application     | 用户建立在Spark之上的程序。包含driver和executors             |
| Application jar | 一个jar中包含用户的Spark Application.                        |
| Driver Program  | 在应用程序中运行`main()`方法，并创建SparkContext             |
| Cluster Manager | 一个外部服务，用来获取资源的                                 |
| Deploy mode     | 设置driver程序运行在哪。client模式运行在集群外部，cluster模式运行在集群内部 |
| Worker node     | 集群内能运行appliciton code的机器                            |
| Executor        | 在Worker node中为application启动的进程，它可以运行tasks,并将数据保存到内存或者磁盘中。每个appliction都有自己的executors |
| Task            | 发送工作单元给executors去执行                                |
| Job             | 并行的计算，一个job中包含多个task 。只要Spark中遇到一个action就是一个job |
| Stage           | 每个作业被划分为更小的任务集，称为阶段，这些阶段相互依赖(类似于MapReduce中的map和reduce阶段) |

### 架构说明

#### 组件

Spark 应用程序是集群上的一组独立运行的进程，通过main方法中的SparkContext来协调组织（叫做driver程序）

具体来说，要运行在集群中,SparkContext可以连接到几种类型的资源管理器(Spark自己的standalone资源管理器，mesos或者yarn)，这些资源管理器可以通过application分配资源。一旦连接，spark获取集群中的节点上的那些为你的application执行计算或者存储数据的executors，然后它会发送你的应用代码给executors(通过JAR或者Python文件的形式传入SparkContext)，最后，SparkContext发送tasks让executors去运行

![](http://spark.apache.org/docs/latest/img/cluster-overview.png)

下面这些有助于理解这个架构:

1. 每个application有它自己的executors进程，这些executors进程生命周期横跨整个application，并且以多线程方式运行。这样做有些好处是会阻隔每个application，阻隔体现在调度端(每个driver调度它自己的tasks)和执行端(tasks来自不用应用程序，在不同的JVM上执行)，但是这样意味着不同的spark application，他们之间的数据不能共享，除非你将数据写入到外部数据存储系统
2. Spark对于底层的Cluster Manager是不可知的。只要spark能获取executors进程并且这些进程之间可以互相通信，那么在支持其他application的ClusterManager(比如：Mesos/YARN)上运行他们也很容易
3. driver程序必须在生命周期中监听并接受来自executors传入的连接.这就要求你部署driver程序的那台worker网络必须是通的
4. 由于driver负责在集群中调度tasks，理论上driver程序应该运行在靠近worker的节点，局域网最好。如果你想通过远程方式发送请求，那么最好是给driver开启RPC协议，并且让driver在靠近worker的节点上提交作业。


