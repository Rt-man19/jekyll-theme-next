---
title: Hadoop离线项目之数据清洗
tags:
  - 大数据
  - Hadoop
categories:
  - 大数据
---

# Hadoop离线项目之数据清洗

## Hadoop离线项目通用框架

![](https://s2.ax1x.com/2019/03/27/AavKeS.png)

**数据处理流程**

1）首先，离线项目处理的数据可能不在集群中，也就是外部数据。一般来讲外部数据有存储在Server或者RDBMS(关系型数据库)中,通过相对应的技术手段来采集到Hadoop集群中

2）ETL。 在Hadoop集群内部执行MapReduce，这里的MapReduce的作用是清洗。但是对于这种字段裁剪、行过滤的操作所涉及到操作的只有“分隔符、时间、IP等字段解析"。这一步的具体过程就是

​    ①. Map去Hadoop中取原数据

​    ②. map方法来过滤不符合条件的数据，完成数据清洗

**所以，ETL操作中使用MapReduce时，只用到了Map**

3）把ETL后的数据存储到Hive中（系统层面）





# 代码实现

## 要求

- 假设给定日志文件格式如下  

  ```text
  baidu	CN	A	E	[24/May/2019:08:44:14 +8000]	2	85.234.121.231	-	112.29.213.35:80	0	v2.go2yd.com	GET	http://v1.go2yd.com/user_upload/fkiu864thcx9dyp13evoz75n2lrbagjw0ms.mp4	HTTP/1.1	-	bytes	468321970	TCP_HIT/206	112.29.213.35	video/mp4	17168	16384	-:0	0	0	-	-	-	11451601	-	JSP3/2.0.14	-	-	-	http	-	2	v1.go2yd.com	0.002	25136186	16384	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	1531818470104-11451601-112.29.213.66#2705261172	644514568
  ```

  

- 对日志进行分隔符、字段等解析



## 具体步骤

一、环境搭建(IDEA + Maven)

在pom.xml中的<properties>中添加

```xml
<hadoop.version>2.6.0-cdh5.7.0</hadoop.version>
```

在pom.xml中的<dependencies>中添加

```xml
<dependency>
	<groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>${hadoop.version}</version>
</dependency>
```

在pom.xml中新添加一个<repositories>标签(默认仓库中没有cdh版本的hadoop，添加其他的仓库)

```xml
<repositories>
	<repository>
    	<id>cloudera</id>
        <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
    </repository>
</repositories>
```

二、解析日志

### 1.创建常用包

我的目录结构如下

![1553670944428](https://s2.ax1x.com/2019/03/27/Aa7gpT.png)



### 2.解析日志的代码

```java
package com.zhuihua.hadoop.utils;

//  日志文件解析对日志的字段进行处理
//  分隔符为\t
public class LogUtil {

    public String parse(String log){
        String result = "";

        return result;
    }
}
```

此时可以先写个单元测试看一下分割后的字段





```java
// src/test/java/com.zhuihua.hadoop.utils/TestLogUtil

package com.zhuihua.hadoop.utils;

import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class TestLogUtil {

    private LogUtil utils;
    
    @Before
    public void SetUp (){
        utils = new LogUtil();
    }

    @After
    public void tearDown (){
        utils = null;
    }

    @Test
    public void testLogParse(){
        String log = "baidu\tCN\tA\tE\t[24/May/2019:08:44:14 +8000]\t2\t85.234.121.231\t-\t112.29.213.35:80\t0\tv2" +
                ".go2yd.com\tGET\thttp://v1.go2yd.com/user_upload/fkiu864thcx9dyp13evoz75n2lrbagjw0ms.mp4\tHTTP/1.1\t-\tbytes\t468321970\tTCP_HIT/206\t112.29.213.35\tvideo/mp4\t17168\t16384\t-:0\t0\t0\t-\t-\t-\t11451601\t-\tJSP3/2.0.14\t-\t-\t-\thttp\t-\t2\tv1.go2yd.com\t0.002\t25136186\t16384\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t-\t1531818470104-11451601-112.29.213.66#2705261172\t644514568";
        String result = utils.parse(log);
        System.out.println(result);
    }
}
```

Debug运行可以看出一条日志一共分割了74个字段

![](https://s2.ax1x.com/2019/03/27/AaOHyT.png)

现在根据选择需要的字段，完善parse这个方法

```java
public class LogUtil {

    public String parse(String log){
        String result = "";

        String[] splits = log.split("\t");
        String cdn = splits[0];
        String region = splits[1];
        String level = splits[3];
        String time = splits[4];
        String ip = splits[6];
        String domain = splits[10];
        String url = splits[12];
        String traffic = splits[16];

        System.out.println(cdn);
        System.out.println(region);
        System.out.println(level);
        System.out.println(time);
        System.out.println(ip);
        System.out.println(domain);
        System.out.println(url);
        System.out.println(traffic);
        return result;
    }
}

```

再次运行单元测试看看是否是想要的结果:

![](https://s2.ax1x.com/2019/03/27/AaXvDS.md.png)

可以看到单元测试已经成功运行，并且获取的字段也是我想要的

接下来处理一下时间字段的格式保留24/May/2019:08:44:14

```java
public class LogUtil {
    public String parse (String log) {
     	String result = "";
        
        String[] splits = log.split("\t");
        ...
        String timeStr = splits[4];
        String time = timeStr.substring(1,timeStr.length()-7);
        
        
        Systemctl.out.println(time);
    }
}
```

![](https://s2.ax1x.com/2019/03/27/AajBxP.md.png)

```java

```

这个输出比刚才的好看一点，但还是可以再进一步处理:

```java
	import java.text.DateFormat;
	import java.util.Locale;
	import java.text.ParseException;
    import java.text.SimpleDateFormat;

	public class LogUtil {
    DateFormat sourceFormat = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.ENGLISH);
    DateFormat targetFormat = new SimpleDateFormat("yyyyMMddHHmmss");
    public String parse(String log){
        
        // 直接输出会抛出异常..
        try {
            String[] splits = log.split("\t");
            String timeStr = splits[4];
            String time = timeStr.substring(1,timeStr.length()-7);
            time = targetFormat.format(sourceFormat.parse(time));
            System.out.println(time);
        } catch (ParseException e){
            e.printStackTrace();
        }
    }
```

再来单元测试打印一下输出看看

![](https://s2.ax1x.com/2019/03/29/A01HTf.png)



### 3. 格式化日志

其实在第2步中日志解析已经完成了，但是日志解析最终的目的是**给外部表使用**，所以解析出来的日志，还有一个分割符问题。

```java
...
...
    try {
        StringBuilder  builder = new StringBuilder();
        builder.append(cdn).append("\t")
				.append(region).append("\t")
           		.append(level).append("\t")
            	.append(time).append("\t")
            	.append(ip).append("\t")
            	.append(domain).append("\t")
           		.append(url).append("\t")
            	.append(traffic);
        result = builder.toString();
    } catch (...) {
     	   ...
    }
...
...
  return result
}

```

### 4.  基于MapReduce的实现

在第三步中已经成功的将一条日志解析出来并转换为方便查看的格式，在这一步中将要实现一个MapReduce方法

```java
package com.zhuihua.hadoop.mapreduce.mapper;

import com.zhuihua.hadoop.utils.LogUtil;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class LogETLMapper extends Mapper<LongWritable, Text, NullWritable,Text> {

    
    //重写map方法
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        int length = value.toString().split("\t").length;
		//判断，如果进来的一条数据不是74个字段，说明不是我们日志中的数据，不作处理
        if (length == 74) {
            LogUtil utils = new LogUtil();
            String result = utils.parse(value.toString());
            // 判断result是否为空
            if (StringUtils.isNotBlank(result)) {
                context.write(NullWritable.get(), new Text(result));
            }

        }
    }
}
```

之前说过，**ETL中只用到了map,没有用到reduce**, 所以还要再开发一个driver 作为程序的入口

```java
package com.zhuihua.hadoop.mapreduce.driver;

import com.zhuihua.hadoop.mapreduce.mapper.LogETLMapper;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class LogETLDriver {

    public static void main(String[] args) throws Exception{
        if(args.length != 2) {
            System.err.println("please input 2 params: input output");
            System.exit(0);
        }

        String input = args[0];
        String output = args[1];  //output/d=20190324
		
        //windows 用户要取消下面这条注释，不然会报空指针这个错误
        //System.setProperty("hadoop.home.dir", "D:/cdh/hadoop-2.6.0-cdh5.7.0");


        Configuration configuration = new Configuration();

        
        FileSystem fileSystem = FileSystem.get(configuration);
        Path outputPath = new Path(output);
        if(fileSystem.exists(outputPath)) {
            fileSystem.delete(outputPath, true);
        }

        Job job = Job.getInstance(configuration);
        job.setJarByClass(LogETLDriver.class);
        job.setMapperClass(LogETLMapper.class);
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(Text.class);

        FileInputFormat.setInputPaths(job, new Path(input));
        FileOutputFormat.setOutputPath(job, new Path(output));

        job.waitForCompletion(true);
    }
}
					
```

### 5.服务器端测试

打包放到服务器端来测试一下

```shell
hadoop jar zk-1.0.jar com.zhuihua.hadoop.mapreduce.driver.LogETLDriver /test/hadoop/accesslog/20190324 /test/hadoop/output/
```

执行完后可以到YARN界面看到有作业运行

![](https://s2.ax1x.com/2019/03/30/ABsopd.png)

等待执行完成后，查看目标目录

![](https://s2.ax1x.com/2019/03/30/ABshkD.png)

使用脚本

```shell
## /home/hadoop/zk-hadoop.sh

process_date=20190324

echo "step 1: mapreduce etl"
hadoop jar zk-1.0.jar com.zhuihua.hadoop.mapreduce.driver.LogETLDriver /test/hadoop/accesslog/$process_date /test/hadoop/output/day=$process_date

echo "step 2: move data to DW"
hadoop fs -mv /test/hadoop/output/day=$process_date/part-r-00000 /test/hadoop/access/clear/day=$process_date/

echo "step 3: alter metadata"
hive -e "create database if not exists hadoop; use hadoop; create external table zk_access (cdn string,region string,level string,time string,ip string,domain string,url string,traffic bigint) partitioned by (day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION '/g6/hadoop/access/clear'; alter table zk_access add if not exists partition(day='$process_date')"
```